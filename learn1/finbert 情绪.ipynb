{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "54c64f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import pipeline\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc4220a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "?pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7c00a1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"/home/liubinxu/下载/FinBERT_L-12_H-768_A-12_pytorch/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "78f451f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/liubinxu/下载/FinBERT_L-12_H-768_A-12_pytorch/ were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/liubinxu/下载/FinBERT_L-12_H-768_A-12_pytorch/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "finbert = BertForSequenceClassification.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e4d66820",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce1730fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizer(name_or_path='/home/liubinxu/下载/FinBERT_L-12_H-768_A-12_pytorch/', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "888ca00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = pipeline(\"sentiment-analysis\", model=finbert, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aedc5e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"明天大涨\",  \n",
    "             \"大跌， 风险将至\"]\n",
    "results = nlp(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2271c1c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_1', 'score': 0.6478438973426819},\n",
       " {'label': 'LABEL_1', 'score': 0.5857692956924438}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "37f82244",
   "metadata": {},
   "outputs": [],
   "source": [
    "easy_money2 = pd.read_table(\"../easymoney_data/002.clean.text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4e3e567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "easy_money2 = easy_money2.rename(columns={\"阅读\": \"read_num\", \"评论\": \"commit_num\", \"标题\": \"title\", \"最后更新\": \"date\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e08f5d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "choose = list(easy_money2.title)[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1452b804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['即使生活再不如意，新的一年到了，祝所有的股友们新年',\n",
       " '过去五年，IPO数量和融资规模分别为：2017年438家，融',\n",
       " '股市是最跟随自然节奏的，最不顺应自然的地方，所以股',\n",
       " ' 希望2023年水平能更进一步，我炒股满两年的时间了，可',\n",
       " '2003',\n",
       " '2o22年走完最后一步。2023年迈开第一步。喜迎开门大吉',\n",
       " ' 新年快乐！加油～[爱心][献花]',\n",
       " '今天要过去了，今年也要过去了。',\n",
       " ' 天使投资人郭涛接受《华夏时报》记者采访表示，受俄乌',\n",
       " '2023许个愿望吧，我先来，1祝家人们身体健康，万事如',\n",
       " '股神附体[大笑]',\n",
       " '新年快乐老铁们',\n",
       " '2023收益翻十倍',\n",
       " ' 跨年夜告诉大家一个好消息一个坏消息！2021年和2021年',\n",
       " '新年快乐！祝大家新的一年能回本！并赚钱！',\n",
       " ' 新年快乐，愿各位今年回本的回本，翻倍的翻倍，爆赚！',\n",
       " '新年快乐，新的一年继续薅主力的羊毛。',\n",
       " '各位股市老铁，兔年来了。祝老铁们兔年吉祥如意，个个',\n",
       " '大盘会涨吗？',\n",
       " '2023发发发']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9fa6af43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.5698588490486145},\n",
       " {'label': 'LABEL_0', 'score': 0.6250249743461609},\n",
       " {'label': 'LABEL_0', 'score': 0.5677523612976074},\n",
       " {'label': 'LABEL_1', 'score': 0.5289913415908813},\n",
       " {'label': 'LABEL_0', 'score': 0.5779617428779602},\n",
       " {'label': 'LABEL_0', 'score': 0.5910698175430298},\n",
       " {'label': 'LABEL_1', 'score': 0.517271101474762},\n",
       " {'label': 'LABEL_0', 'score': 0.5909019112586975},\n",
       " {'label': 'LABEL_0', 'score': 0.5499157309532166},\n",
       " {'label': 'LABEL_0', 'score': 0.5286133289337158},\n",
       " {'label': 'LABEL_0', 'score': 0.5200566649436951},\n",
       " {'label': 'LABEL_0', 'score': 0.5232834219932556},\n",
       " {'label': 'LABEL_0', 'score': 0.5761458873748779},\n",
       " {'label': 'LABEL_0', 'score': 0.5734653472900391},\n",
       " {'label': 'LABEL_0', 'score': 0.528021514415741},\n",
       " {'label': 'LABEL_0', 'score': 0.5198492407798767},\n",
       " {'label': 'LABEL_0', 'score': 0.5467403531074524},\n",
       " {'label': 'LABEL_1', 'score': 0.502379834651947},\n",
       " {'label': 'LABEL_0', 'score': 0.5026551485061646},\n",
       " {'label': 'LABEL_0', 'score': 0.6048484444618225}]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(list(choose))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f427c02f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1251300/3954467170.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0measy_money2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/pipelines/text_classification.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0mIf\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone\u001b[0m \u001b[0msuch\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m \u001b[0mper\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \"\"\"\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0;31m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0m_legacy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"top_k\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1063\u001b[0m                     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m                 )\n\u001b[0;32m-> 1065\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfinal_iterator\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1066\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1063\u001b[0m                     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m                 )\n\u001b[0;32m-> 1065\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfinal_iterator\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1066\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/pipelines/pt_utils.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# We're out of items within a batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;31m# We now have a batch of \"inferred things\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/pipelines/pt_utils.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# We're out of items within a batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0;31m# We now have a batch of \"inferred things\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader_batch_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m    990\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m                     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 992\u001b[0;31m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    993\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/pipelines/text_classification.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, model_inputs)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_to_apply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_legacy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1561\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1563\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m   1564\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1565\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1017\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m         )\n\u001b[0;32m-> 1019\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1020\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    607\u001b[0m                 )\n\u001b[1;32m    608\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    610\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    535\u001b[0m             \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcross_attn_present_key_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m         layer_output = apply_chunking_to_forward(\n\u001b[0m\u001b[1;32m    538\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/pytorch_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = nlp(list(easy_money2.title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bd2edb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_score_list = [x['score'] for x in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7ca91f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "easy_money2[\"sentiments\"] = results_score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cae966b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "easy_money2.to_csv(\"002.senti.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ade930e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "easy_money2[\"date\"] = easy_money2[\"date\"].map(lambda x: x.split(\" \")[0])\n",
    "nums = easy_money2[\"sentiments\"].groupby(easy_money2[\"date\"]).count()\n",
    "sums = easy_money2[\"sentiments\"].groupby(easy_money2[\"date\"]).sum()\n",
    "day_senti = pd.DataFrame()\n",
    "day_senti[\"num\"] = nums\n",
    "day_senti[\"sum\"] = sums\n",
    "day_senti[\"senti_value\"] = sums/nums\n",
    "day_senti.to_csv(\"2023_easy_money_sens_bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bfa531a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "choose3 = [\"上证指数的走势被港股带回原有的反弹轨道，但随着港股今日回落，A股也跟着回落了。\",\n",
    "\n",
    "    \"假期外围普涨，港股反弹创了新高，越过左肩高点，美股假期也是持续反弹，在特斯拉的带动下造车新势力普涨，因此今日A股汽车类个股全线反弹。\",\n",
    "\n",
    "    \"A股始终就是跟着别人走，看别人脸色。\",\n",
    "\n",
    "    \"现实中的二级市场远比车企以及某站车评人吹嘘来的现实的多。\",\n",
    "\n",
    "     \"2月份整体是震荡的走势，也就是说指数要构筑一个中枢扩展的结构才能再上。\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8f2557d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_1', 'score': 0.6562941074371338},\n",
       " {'label': 'LABEL_1', 'score': 0.6379492282867432},\n",
       " {'label': 'LABEL_1', 'score': 0.5668553709983826},\n",
       " {'label': 'LABEL_1', 'score': 0.5000359416007996},\n",
       " {'label': 'LABEL_1', 'score': 0.6239464282989502}]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(choose3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5bba43f1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AutoTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1251300/1921299879.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bert-base-chinese\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'AutoTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d69653",
   "metadata": {},
   "outputs": [],
   "source": [
    "#默认bert中文模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "09570702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.04229164123535156,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 38,
       "postfix": null,
       "prefix": "Downloading (…)okenizer_config.json",
       "rate": null,
       "total": 29,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97f2a65f232e4f35820adfe499f488e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.040976524353027344,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 38,
       "postfix": null,
       "prefix": "Downloading (…)lve/main/config.json",
       "rate": null,
       "total": 624,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b48fca96cdcd4bfbbc9cbef62655b5c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/624 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.016046524047851562,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 38,
       "postfix": null,
       "prefix": "Downloading (…)solve/main/vocab.txt",
       "rate": null,
       "total": 109540,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2966439942aa47f4894c29b680068f63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01802229881286621,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 38,
       "postfix": null,
       "prefix": "Downloading (…)/main/tokenizer.json",
       "rate": null,
       "total": 268943,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8430e0b111654d3da738cb0b59d21807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/269k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03148484230041504,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 38,
       "postfix": null,
       "prefix": "Downloading (…)\"pytorch_model.bin\";",
       "rate": null,
       "total": 411577189,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e98c222e1cee4140a3a4c80feb54bf21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/412M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"bert-base-chinese\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f2d1cf94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'BertForMaskedLM' is not supported for sentiment-analysis. Supported models are ['AlbertForSequenceClassification', 'BartForSequenceClassification', 'BertForSequenceClassification', 'BigBirdForSequenceClassification', 'BigBirdPegasusForSequenceClassification', 'BloomForSequenceClassification', 'CamembertForSequenceClassification', 'CanineForSequenceClassification', 'ConvBertForSequenceClassification', 'CTRLForSequenceClassification', 'Data2VecTextForSequenceClassification', 'DebertaForSequenceClassification', 'DebertaV2ForSequenceClassification', 'DistilBertForSequenceClassification', 'ElectraForSequenceClassification', 'ErnieForSequenceClassification', 'EsmForSequenceClassification', 'FlaubertForSequenceClassification', 'FNetForSequenceClassification', 'FunnelForSequenceClassification', 'GPT2ForSequenceClassification', 'GPT2ForSequenceClassification', 'GPTNeoForSequenceClassification', 'GPTJForSequenceClassification', 'IBertForSequenceClassification', 'LayoutLMForSequenceClassification', 'LayoutLMv2ForSequenceClassification', 'LayoutLMv3ForSequenceClassification', 'LEDForSequenceClassification', 'LiltForSequenceClassification', 'LongformerForSequenceClassification', 'LukeForSequenceClassification', 'MarkupLMForSequenceClassification', 'MBartForSequenceClassification', 'MegatronBertForSequenceClassification', 'MobileBertForSequenceClassification', 'MPNetForSequenceClassification', 'MvpForSequenceClassification', 'NezhaForSequenceClassification', 'NystromformerForSequenceClassification', 'OpenAIGPTForSequenceClassification', 'OPTForSequenceClassification', 'PerceiverForSequenceClassification', 'PLBartForSequenceClassification', 'QDQBertForSequenceClassification', 'ReformerForSequenceClassification', 'RemBertForSequenceClassification', 'RobertaForSequenceClassification', 'RobertaPreLayerNormForSequenceClassification', 'RoCBertForSequenceClassification', 'RoFormerForSequenceClassification', 'SqueezeBertForSequenceClassification', 'TapasForSequenceClassification', 'TransfoXLForSequenceClassification', 'XLMForSequenceClassification', 'XLMRobertaForSequenceClassification', 'XLMRobertaXLForSequenceClassification', 'XLNetForSequenceClassification', 'YosoForSequenceClassification'].\n"
     ]
    }
   ],
   "source": [
    "nlp2 = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "69271522",
   "metadata": {},
   "outputs": [],
   "source": [
    "choose = list(easy_money2.title)[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4e5476ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['即使生活再不如意，新的一年到了，祝所有的股友们新年',\n",
       " '过去五年，IPO数量和融资规模分别为：2017年438家，融',\n",
       " '股市是最跟随自然节奏的，最不顺应自然的地方，所以股',\n",
       " ' 希望2023年水平能更进一步，我炒股满两年的时间了，可',\n",
       " '2003',\n",
       " '2o22年走完最后一步。2023年迈开第一步。喜迎开门大吉',\n",
       " ' 新年快乐！加油～[爱心][献花]',\n",
       " '今天要过去了，今年也要过去了。',\n",
       " ' 天使投资人郭涛接受《华夏时报》记者采访表示，受俄乌',\n",
       " '2023许个愿望吧，我先来，1祝家人们身体健康，万事如',\n",
       " '股神附体[大笑]',\n",
       " '新年快乐老铁们',\n",
       " '2023收益翻十倍',\n",
       " ' 跨年夜告诉大家一个好消息一个坏消息！2021年和2021年',\n",
       " '新年快乐！祝大家新的一年能回本！并赚钱！',\n",
       " ' 新年快乐，愿各位今年回本的回本，翻倍的翻倍，爆赚！',\n",
       " '新年快乐，新的一年继续薅主力的羊毛。',\n",
       " '各位股市老铁，兔年来了。祝老铁们兔年吉祥如意，个个',\n",
       " '大盘会涨吗？',\n",
       " '2023发发发']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7d2b1019",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "106726",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1251300/1958734826.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m results2 = nlp2(choose\n\u001b[0m\u001b[1;32m      2\u001b[0m                )\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/pipelines/text_classification.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0mIf\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone\u001b[0m \u001b[0msuch\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m \u001b[0mper\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \"\"\"\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0;31m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0m_legacy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"top_k\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1063\u001b[0m                     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m                 )\n\u001b[0;32m-> 1065\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfinal_iterator\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1066\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1063\u001b[0m                     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m                 )\n\u001b[0;32m-> 1065\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfinal_iterator\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1066\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/pipelines/pt_utils.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# We're out of items within a batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0;31m# We now have a batch of \"inferred things\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader_batch_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/pipelines/text_classification.py\u001b[0m in \u001b[0;36mpostprocess\u001b[0;34m(self, model_outputs, function_to_apply, top_k, _legacy)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtop_k\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_legacy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"score\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         dict_scores = [\n",
      "\u001b[0;31mKeyError\u001b[0m: 106726"
     ]
    }
   ],
   "source": [
    "results2 = nlp2(choose\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "711dbbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir2 = \"/liubinxu/liubinxu/finance/sentiment_cls/models/albert_chinese_small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "868e057c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/liubinxu/下载/FinBERT_L-12_H-768_A-12_pytorch/ were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/liubinxu/下载/FinBERT_L-12_H-768_A-12_pytorch/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "finbert2 = BertForSequenceClassification.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3747aa20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'AlbertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer2 = BertTokenizer.from_pretrained(model_dir2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a56ea809",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp2 = pipeline(\"sentiment-analysis\", model=finbert, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "638e64c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.text_classification.TextClassificationPipeline at 0x7f1102432430>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "57227458",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.5698588490486145},\n",
       " {'label': 'LABEL_0', 'score': 0.6250249743461609},\n",
       " {'label': 'LABEL_0', 'score': 0.5677523612976074},\n",
       " {'label': 'LABEL_1', 'score': 0.5289913415908813},\n",
       " {'label': 'LABEL_0', 'score': 0.5779617428779602},\n",
       " {'label': 'LABEL_0', 'score': 0.5910698175430298},\n",
       " {'label': 'LABEL_1', 'score': 0.517271101474762},\n",
       " {'label': 'LABEL_0', 'score': 0.5909019112586975},\n",
       " {'label': 'LABEL_0', 'score': 0.5499157309532166},\n",
       " {'label': 'LABEL_0', 'score': 0.5286133289337158},\n",
       " {'label': 'LABEL_0', 'score': 0.5200566649436951},\n",
       " {'label': 'LABEL_0', 'score': 0.5232834219932556},\n",
       " {'label': 'LABEL_0', 'score': 0.5761458873748779},\n",
       " {'label': 'LABEL_0', 'score': 0.5734653472900391},\n",
       " {'label': 'LABEL_0', 'score': 0.528021514415741},\n",
       " {'label': 'LABEL_0', 'score': 0.5198492407798767},\n",
       " {'label': 'LABEL_0', 'score': 0.5467403531074524},\n",
       " {'label': 'LABEL_1', 'score': 0.502379834651947},\n",
       " {'label': 'LABEL_0', 'score': 0.5026551485061646},\n",
       " {'label': 'LABEL_0', 'score': 0.6048484444618225}]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp2(list(choose))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9fd1a5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"/liubinxu/liubinxu/finance/sentiment_cls/input/data01/test1\"\n",
    "with open(data, 'r') as f:\n",
    "    data_list = f.readlines()\n",
    "data_list = [x.strip() for x in data_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b6a7a2e5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['仁东控股控股股东及一致行动人拟被动减持不超过公司股份\\n',\n",
       " '青岛啤酒复星集团减持公司万股股占总股本约\\n',\n",
       " '隆基股份李春安于月日至日累计减持公司股份\\n',\n",
       " '芯源微两股东拟合计减持不超公司股份\\n',\n",
       " '福耀玻璃河仁慈善基金会拟减持不超过万股公司股股份\\n',\n",
       " '华大基因股东减持股份比例累计达到\\n',\n",
       " '迪安诊断第一期员工持股计划股份出售完毕\\n',\n",
       " '建研院股东冯国宝拟减持不超过公司股份\\n',\n",
       " '西藏珠峰因信托计划到期西藏信托减持公司股份\\n',\n",
       " '长阳科技陶春风及其一致行动人减持公司股份\\n',\n",
       " '华谊嘉信杭州福石资产管理有限公司减持公司股份\\n',\n",
       " '完美世界公司第一期员工持股计划存续期届满暨计划终止卖出约万股\\n',\n",
       " '未名医药高宝林减持公司股份\\n',\n",
       " '迪安诊断第一期员工持股计划股份出售完毕暨计划终止卖出万股\\n',\n",
       " '特力远致富海减持公司股份\\n',\n",
       " '驰宏锌锗苏庭宝减持公司股份\\n',\n",
       " '上海莱士控股股东科瑞天诚投资控股有限公司及一致行动人被动减持公司股份\\n',\n",
       " '宏发股份联发集团减持公司股份\\n',\n",
       " '景峰医药股东叶湘武拟减持不超过公司股份\\n',\n",
       " '西部超导深圳市创新投资集团有限公司减持股份\\n',\n",
       " '震安科技广发信德减持公司股份\\n',\n",
       " '星帅尔控股股东实际控制人楼月根减持公司股份\\n',\n",
       " '荣晟环保控股股东实际控制人之一的冯荣华减持比例达\\n',\n",
       " '天奈科技减持达到\\n',\n",
       " '科林电气控股股东及一致行动人减持公司股份\\n',\n",
       " '天创时尚香港高创减持公司股份\\n',\n",
       " '威派格李纪玺及其一致行动人减持公司可转债\\n',\n",
       " '星宇股份周晓萍减持公司可转债\\n',\n",
       " '海航科技被问询要求说明公司资产置入及本次出售之间关系\\n',\n",
       " '合众思壮收到关注函要求对比说明思壮北斗投资性房地产评估增值的合理性\\n',\n",
       " '中国巨石决定终止筹划重大资产重组事项股票复牌\\n',\n",
       " '中材科技决定终止筹划重大资产重组事项股票复牌\\n',\n",
       " '中芯国际一份美国民事诉状将公司及部分董事列为被告将作出积极抗辩\\n',\n",
       " '仁东控股控股股东及其一致行动人两融业务有触发强制平仓的可能性\\n',\n",
       " '仁东控股京基集团在涨停板减持约公司股份\\n',\n",
       " '东风汽车股票连续涨停东风集团首发过会对公司生产经营不构成重大影响\\n',\n",
       " '江淮汽车建投投资拟减持不超过总股本\\n',\n",
       " '每日互动浙江证监局拟对公司开展现场检查\\n',\n",
       " '中达安实际控制人拟转让公司股份\\n',\n",
       " '海特高新控股股东及一致行动人拟亿元转让公司股份\\n',\n",
       " '北方华创国家集成电路基金累计减持比例达总股本\\n',\n",
       " '顶点软件金石投资拟减持不超过总股本\\n',\n",
       " '金溢科技大股东王明宽及其一致行动人拟减持不超过公司股份\\n',\n",
       " '汇嘉时代持股股东鑫源汇信拟清仓式减持\\n',\n",
       " '华体科技持股股东东吴创投拟清仓式减持\\n',\n",
       " '金力泰收关注函深交所要求披露就刘少林涉嫌合同诈骗的具体事项所掌握的信息\\n',\n",
       " '常铝股份收关注函深交所要求说明签订新冠疫苗产业化净化安装工程合同是否存在蹭热点炒作公司股价以配合股东减持的情形\\n',\n",
       " '亿纬锂能收关注函深交所要求公司说明拟获得亿纬集能股权比例的合理性\\n',\n",
       " '南方航空月客运运力投入同比下降旅客周转量同比下降\\n',\n",
       " '东方航空月客运运力投入同比下降旅客周转量同比下降\\n',\n",
       " '中国国航月客运运力投入同比下降环比下降旅客周转量同比下降环比下降\\n',\n",
       " '中潜股份实控人仰智慧涉嫌操纵证券市场遭证监会立案调查\\n',\n",
       " '仁东控股连续日跌停收到深交所关注函深交所要求说明崇左中烁买卖公司股票的具体情况\\n',\n",
       " '六连板朗姿股份拟出售韩国化妆品公司剩余全部股权\\n',\n",
       " '申华控股自查发现亿元的违规对外担保沈阳广泰是否会按期或提前还款尚不能确\\n',\n",
       " '金辰股份公司并不涉及光伏组件及光伏电池片的生产及销售不属于光伏行业\\n',\n",
       " '大洲乌拉圭参股子公司确诊三起新冠病例\\n',\n",
       " '每日互动员工涉嫌虚增销售合同侵害公司利益并涉嫌违法犯罪\\n',\n",
       " '中来股份子公司技术进展对公司未来的利润影响有限\\n',\n",
       " '两连板悦心健康医疗美容科的营收金额很小\\n',\n",
       " '吉祥航空月客运运力投入同比下降旅客周转量同比下降\\n',\n",
       " '江淮汽车减持计划实施完毕建投投资累计减持股份\\n',\n",
       " '金力泰第一大流通股股东的股东刘少林被刑事拘留\\n',\n",
       " '瀚叶子公司可能存在应收账款大额计提减值的情况\\n',\n",
       " '传艺科技控股股东实际控制人邹伟民计划减持不超公司股份\\n',\n",
       " '神力股份股东拟合计减持公司不超股份\\n',\n",
       " '高升股东拟减持不超公司股份\\n',\n",
       " '泽璟制药索拉非尼纳入集采预计对公司产品多纳非尼的未来市场份额产生影响\\n',\n",
       " '江苏北人多名股东及董监高拟合计减持不超公司股份其中涌控投资或清仓减持股份\\n',\n",
       " '乐惠国际股东及董监高拟合计减持不超公司股份\\n',\n",
       " '晶晨股份集中竞价交易减持数量过半王牌减持公司股份\\n',\n",
       " '中孚将被实施退市风险警示停牌一天\\n',\n",
       " '科融环境收到证监会行政处罚决定书\\n',\n",
       " '劲嘉股份广田集团外部董事白涛配合公安机关协助调查\\n',\n",
       " '华鹏飞股东杨阳所持公司股份将被司法处置\\n',\n",
       " '索菱公司及相关当事人收到行政处罚决定书及市场禁入决定书\\n',\n",
       " '广信材料收到关注函要求补充披露为本次股权转让出具的东莞航盛相关审计报告\\n',\n",
       " '辉丰收到关注函要求说明年产吨草铵膦原药生产线技改项目的权属\\n',\n",
       " '东洋收到关注函要求说明与公司对业绩承诺补偿款认定是否存纠纷\\n',\n",
       " '伊利股份第五期持股计划股票出售完毕共卖出万股\\n',\n",
       " '宏辉果蔬控股股东累计减持股份完成减持计划\\n',\n",
       " '实丰文化控股股东蔡俊权减持公司股份\\n',\n",
       " '密尔克卫股东拟减持公司不超股份\\n',\n",
       " '金域医学君睿祺及其一致行动人拟清仓减持不超过公司股份\\n',\n",
       " '华信新材特定股东苏州国发减持公司股份\\n',\n",
       " '天际股份新华化工新昊投资累计减持公司股份\\n',\n",
       " '铂力特西安西北工业大学资产经营管理有限公司减持达到\\n',\n",
       " '天马股东拟被动减持公司不超股份\\n',\n",
       " '雪榕生物股东余荣琳拟减持不超公司股份\\n',\n",
       " '闻泰科技西藏风格西藏富恒鹏欣智澎合计减持公司股份\\n',\n",
       " '仁东控股公司目前经营情况及内外部经营环境未发生重大变化\\n',\n",
       " '大湖股份连续三日涨停提示股票交易风险白酒产品占公司营收比重较小\\n',\n",
       " '龙大肉食龙大养殖月生猪销售收入亿元环比减少\\n',\n",
       " '康泰生物股东拟减持不超过总股本\\n',\n",
       " '金安国纪回复关注函不存在违反信息披露公平原则\\n',\n",
       " '陕西黑猫公司披露的子公司内蒙古黑猫规划建设项目存在不确定性风险\\n',\n",
       " '威龙公司将变为无实际控制人\\n',\n",
       " '东洋李兴祥无法按期支付业绩承诺补偿款\\n',\n",
       " '英飞特华睿泰信尚志投资拟合计减持不超过公司股份\\n',\n",
       " '保力新股东郭鸿宝拟被动减持不超过公司股份\\n',\n",
       " '上海新阳首次回购股份万股\\n',\n",
       " '方大集团首次回购股股份万股\\n',\n",
       " '太平鸟首次回购股份万股\\n',\n",
       " '宁波建工子公司联合中标镇海区亿元工程总承包\\n',\n",
       " '尚纬股份近期累计中标亿元\\n',\n",
       " '东华软件公司与腾讯云联合中标浦城县新型智慧城市建设项目一期服务类采购项目\\n',\n",
       " '榕基软件中标海关总署万元采购项目\\n',\n",
       " '西王食品预计年净利同比增\\n',\n",
       " '中国联通月用户净增万户累计亿户\\n',\n",
       " '中工国际控股股东成为国有资本投资公司试点企业\\n',\n",
       " '雅戈尔处置部分金融资产增加净利近万元\\n',\n",
       " '凯盛科技实际控制人中国建材集团被确定为国有资本投资公司试点企业\\n',\n",
       " '乐金健康公司名称和证券简称变更\\n',\n",
       " '现代制药控股子公司国药一心收到吉林省药品监督管理局颁发的药品证书\\n',\n",
       " '开立医疗三种体外诊断试剂产品已获广东省药品监督管理局批准\\n',\n",
       " '药明康德拟向全资子公司上海药明增资亿元现金\\n',\n",
       " '鲁抗医药拟竞购华鲁控股持有的赛特公司的股权\\n',\n",
       " '龙元建设中标杭州市城市档案中心项目\\n',\n",
       " '广信材料拟万增资揽入航盛新能源股权\\n',\n",
       " '同洲电子获华夏人寿举牌其持股达到\\n',\n",
       " '绿庭投资减持申万宏源万股获益约万元\\n',\n",
       " '志邦家居拟开展外汇衍生品交易业务\\n',\n",
       " '柯利达预中标西昌市一环路历史风貌核心区二期及城区亮化工程二期项目\\n',\n",
       " '中国重工拟向控股股东出售亏损海工板块资产\\n',\n",
       " '白云山全资子公司天心药业被广东省药监局收回药品证书\\n',\n",
       " '山河智能广州市政府旗下企业拟获取公司控制权\\n',\n",
       " '岭南股份中标渭南市华州区赤水河高塘镇至入渭口段综合治理工程建设项目\\n',\n",
       " '弘信电子股东上海金投邱葵王毅拟个月内合计减持股份\\n',\n",
       " '通合科技重组有条件过会月日复牌\\n',\n",
       " '富安娜拟回购亿元亿元股份\\n',\n",
       " '东方网络东柏文化将成为公司第一大股东\\n',\n",
       " '八菱科技回购期届满累积回购股份\\n',\n",
       " '多喜爱继续推进股权转让事项\\n',\n",
       " '上海电力拟亿元收购国家电投旗下浙江新能源公司\\n',\n",
       " '汇通能源拟出资亿元参与设立青岛蓬晖股权投资合伙企业\\n',\n",
       " '三棵树拟逾亿元收购防水材料公司大禹防漏股权\\n',\n",
       " '晨光生物拟出资万元参股邯郸银行\\n',\n",
       " '财信发展子公司拟联合融创西南公司等收购星界置业股权及债权\\n',\n",
       " '科顺股份拟出资万联合设立控股子公司深圳前海铂盾新材料股份有限公司\\n',\n",
       " '天奇股份子公司拟万元收购锂致实业股权\\n',\n",
       " '迪马股份子公司联合竞得重庆九龙坡区地块\\n',\n",
       " '康德莱管理层提请董事会授权启动登陆香港联交所前期筹备工作\\n',\n",
       " '得润电子拟万元转让得康电子股权\\n',\n",
       " '珠海中富拟出售陕西中富股权料获收益万元\\n',\n",
       " '北方稀土拟亿元向包钢股份转让稀土选矿有关资产\\n',\n",
       " '西南证券出让重庆股份转让中心股权不再合并其财务报表\\n',\n",
       " '集泰股份今日首次回购万股\\n',\n",
       " '旗滨集团拟回购亿元亿元股份\\n',\n",
       " '苏宁环球回购公司股份比例达\\n',\n",
       " '科林电气实控人之一增持万股\\n',\n",
       " '美的集团截至月日累计回购股份\\n',\n",
       " '交通银行墨尔本分行正式开业\\n',\n",
       " '普利制药收到海南药品监督管理局颁发的药品证书\\n',\n",
       " '陇神戎发公司保健食品获得俄罗斯联邦国家注册证书\\n',\n",
       " '银轮股份公司被确定为通用平台水空中冷器定点供应商\\n',\n",
       " '温氏股份广东温氏集团财务有限公司开业获批\\n',\n",
       " '海正药业注射用帕瑞昔布钠获药品注册批件\\n',\n",
       " '三峡新材获批郑商所指定玻璃期货交割厂库\\n',\n",
       " '西水股份子公司天安财险前月保费收入亿元同比增\\n',\n",
       " '华伍股份实控人及其关联人拟转让股份给丰水湖投资\\n',\n",
       " '大名城拟参与开发杭州余杭区地产项目\\n',\n",
       " '深物业拟收购控股股东旗下投控物业股权\\n',\n",
       " '星云股份与比亚迪及其子公司签署设备采购合同\\n',\n",
       " '中航资本拟亿元增资中航证券\\n',\n",
       " '太极集团公司药品藿香正气口服液获得柬埔寨卫生部传统药物注册批文\\n',\n",
       " '北汽蓝谷北京高端智能生态工厂建设项目立项\\n',\n",
       " '南洋科技月日起改名为航天彩虹\\n',\n",
       " '海南矿业拟亿美元收购洛克石油股权积极参与南海石油天然气开发\\n',\n",
       " '赛轮轮胎拟设立合资公司扩大在越南生产规模\\n',\n",
       " '金智科技子公司拟万元转让城建隧桥股权\\n',\n",
       " '中国国旅拟亿元增资国旅投资并参与海口土地竞拍\\n',\n",
       " '岭南股份中标河南新郑市约亿元项目\\n',\n",
       " '中孚实业拟停产林丰铝电并转移产能涉电解铝产能万吨\\n',\n",
       " '恒力股份万吨年炼化一体化项目即将投料开车公司业务向上游延伸\\n',\n",
       " '智动力拟亿元收购阿特斯股权阿特斯成控股子公司\\n',\n",
       " '长川科技拟亿元收购长新投资股权\\n',\n",
       " '中国武夷拟非公开发行不超亿元公司债\\n',\n",
       " '中信重工两家下属公司拟亿元参与中信财务增资\\n',\n",
       " '中南建设拟逾亿元受让控股股东部分房地产业务子公司股权\\n',\n",
       " '潜能恒信与石油大学签订人工智能研究中心共建协议\\n',\n",
       " '新奥股份拟指定境外子公司以万美元现金购买东芝美国天然气公司股权\\n',\n",
       " '精工钢构拟亿元收购墙煌新材料股份\\n',\n",
       " '向日葵拟出售部分光伏设备等资产\\n',\n",
       " '宏图高科控股股东及实控人增持计划延期个月履行\\n',\n",
       " '包钢股份拟以亿元收购北方稀土与稀土选矿有关的实物资产及资源\\n',\n",
       " '海航控股近亿元收购新华航空天津航空少数股权\\n',\n",
       " '盈趣科技获得国家技术创新示范企业认定\\n',\n",
       " '中国武夷拟非公开发行不超亿元公司债补血及加码主业\\n',\n",
       " '瑞康医药拟回购亿元亿元股份\\n',\n",
       " '天山股份对子公司阜康天山以债转股方式进行增资\\n',\n",
       " '众应互联拟以资产置换方式收购瀚德信用股权\\n',\n",
       " '永泰能源明起复牌公司生产经营金融债务等稳定\\n',\n",
       " '景峰医药子公司安非他命混合盐获得美国批准文号\\n',\n",
       " '中国巨石子公司获第五届中国工业大奖\\n',\n",
       " '文化长城与潮州国企战略合作涉大股东股权质押业务\\n',\n",
       " '皇庭国际拟亿元至亿元回购股份\\n',\n",
       " '正泰电器董事陆川完成增持计划\\n',\n",
       " '杰瑞股份与美国财政部外国资产控制办公室签署和解协议\\n',\n",
       " '保利地产前月签约金额亿元同比增长\\n',\n",
       " '唐人神控股股东拟出让股份引入湖南资管\\n']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c19ec390",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nlp2(list(data_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d60c44c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "782d482a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_1', 'score': 0.5013431310653687},\n",
       " {'label': 'LABEL_0', 'score': 0.5273839831352234},\n",
       " {'label': 'LABEL_0', 'score': 0.5132502317428589},\n",
       " {'label': 'LABEL_0', 'score': 0.5328633189201355},\n",
       " {'label': 'LABEL_1', 'score': 0.5181031823158264},\n",
       " {'label': 'LABEL_0', 'score': 0.528239905834198},\n",
       " {'label': 'LABEL_0', 'score': 0.5184571743011475},\n",
       " {'label': 'LABEL_0', 'score': 0.56069415807724},\n",
       " {'label': 'LABEL_0', 'score': 0.5500088334083557},\n",
       " {'label': 'LABEL_0', 'score': 0.5338916182518005},\n",
       " {'label': 'LABEL_0', 'score': 0.5399091243743896},\n",
       " {'label': 'LABEL_1', 'score': 0.5069075226783752},\n",
       " {'label': 'LABEL_0', 'score': 0.5402593016624451},\n",
       " {'label': 'LABEL_0', 'score': 0.547822892665863},\n",
       " {'label': 'LABEL_0', 'score': 0.5520148277282715},\n",
       " {'label': 'LABEL_0', 'score': 0.51774001121521},\n",
       " {'label': 'LABEL_0', 'score': 0.5379172563552856},\n",
       " {'label': 'LABEL_0', 'score': 0.5519105792045593},\n",
       " {'label': 'LABEL_0', 'score': 0.5037607550621033},\n",
       " {'label': 'LABEL_0', 'score': 0.510290801525116},\n",
       " {'label': 'LABEL_0', 'score': 0.5593365430831909},\n",
       " {'label': 'LABEL_0', 'score': 0.5282791256904602},\n",
       " {'label': 'LABEL_0', 'score': 0.5431284308433533},\n",
       " {'label': 'LABEL_0', 'score': 0.5888927578926086},\n",
       " {'label': 'LABEL_0', 'score': 0.5128911137580872},\n",
       " {'label': 'LABEL_0', 'score': 0.5410947203636169},\n",
       " {'label': 'LABEL_0', 'score': 0.5505601167678833},\n",
       " {'label': 'LABEL_0', 'score': 0.5365808606147766},\n",
       " {'label': 'LABEL_0', 'score': 0.5665440559387207},\n",
       " {'label': 'LABEL_0', 'score': 0.5619391798973083},\n",
       " {'label': 'LABEL_1', 'score': 0.5421755313873291},\n",
       " {'label': 'LABEL_1', 'score': 0.5414730310440063},\n",
       " {'label': 'LABEL_0', 'score': 0.632880687713623},\n",
       " {'label': 'LABEL_0', 'score': 0.631888747215271},\n",
       " {'label': 'LABEL_0', 'score': 0.5309782028198242},\n",
       " {'label': 'LABEL_0', 'score': 0.5405016541481018},\n",
       " {'label': 'LABEL_1', 'score': 0.5172325968742371},\n",
       " {'label': 'LABEL_0', 'score': 0.6096330881118774},\n",
       " {'label': 'LABEL_0', 'score': 0.5825401544570923},\n",
       " {'label': 'LABEL_1', 'score': 0.5149098634719849},\n",
       " {'label': 'LABEL_0', 'score': 0.5338725447654724},\n",
       " {'label': 'LABEL_0', 'score': 0.5214585661888123},\n",
       " {'label': 'LABEL_0', 'score': 0.5135409832000732},\n",
       " {'label': 'LABEL_0', 'score': 0.5419949889183044},\n",
       " {'label': 'LABEL_0', 'score': 0.5419912934303284},\n",
       " {'label': 'LABEL_0', 'score': 0.589401125907898},\n",
       " {'label': 'LABEL_0', 'score': 0.5724568963050842},\n",
       " {'label': 'LABEL_0', 'score': 0.5603868365287781},\n",
       " {'label': 'LABEL_0', 'score': 0.6048322319984436},\n",
       " {'label': 'LABEL_0', 'score': 0.6103852391242981},\n",
       " {'label': 'LABEL_0', 'score': 0.5891805291175842},\n",
       " {'label': 'LABEL_0', 'score': 0.5460545420646667},\n",
       " {'label': 'LABEL_0', 'score': 0.5669123530387878},\n",
       " {'label': 'LABEL_0', 'score': 0.5691765546798706},\n",
       " {'label': 'LABEL_0', 'score': 0.5962427854537964},\n",
       " {'label': 'LABEL_0', 'score': 0.5943811535835266},\n",
       " {'label': 'LABEL_0', 'score': 0.5352417826652527},\n",
       " {'label': 'LABEL_0', 'score': 0.5747072696685791},\n",
       " {'label': 'LABEL_0', 'score': 0.558952808380127},\n",
       " {'label': 'LABEL_0', 'score': 0.5900859832763672},\n",
       " {'label': 'LABEL_0', 'score': 0.5953880548477173},\n",
       " {'label': 'LABEL_1', 'score': 0.5241637229919434},\n",
       " {'label': 'LABEL_1', 'score': 0.5010732412338257},\n",
       " {'label': 'LABEL_0', 'score': 0.5879293084144592},\n",
       " {'label': 'LABEL_0', 'score': 0.51401686668396},\n",
       " {'label': 'LABEL_0', 'score': 0.5398727059364319},\n",
       " {'label': 'LABEL_0', 'score': 0.5482982993125916},\n",
       " {'label': 'LABEL_0', 'score': 0.5914380550384521},\n",
       " {'label': 'LABEL_0', 'score': 0.5129920840263367},\n",
       " {'label': 'LABEL_0', 'score': 0.5050792098045349},\n",
       " {'label': 'LABEL_0', 'score': 0.5526615381240845},\n",
       " {'label': 'LABEL_0', 'score': 0.5671754479408264},\n",
       " {'label': 'LABEL_0', 'score': 0.5280151963233948},\n",
       " {'label': 'LABEL_0', 'score': 0.612386167049408},\n",
       " {'label': 'LABEL_0', 'score': 0.5131939053535461},\n",
       " {'label': 'LABEL_0', 'score': 0.5416576266288757},\n",
       " {'label': 'LABEL_0', 'score': 0.5578381419181824},\n",
       " {'label': 'LABEL_0', 'score': 0.5256161093711853},\n",
       " {'label': 'LABEL_0', 'score': 0.6073434352874756},\n",
       " {'label': 'LABEL_1', 'score': 0.5024906992912292},\n",
       " {'label': 'LABEL_0', 'score': 0.5306412577629089},\n",
       " {'label': 'LABEL_0', 'score': 0.5119685530662537},\n",
       " {'label': 'LABEL_0', 'score': 0.5491047501564026},\n",
       " {'label': 'LABEL_0', 'score': 0.5117009282112122},\n",
       " {'label': 'LABEL_0', 'score': 0.5507673621177673},\n",
       " {'label': 'LABEL_0', 'score': 0.5446344614028931},\n",
       " {'label': 'LABEL_0', 'score': 0.5125354528427124},\n",
       " {'label': 'LABEL_0', 'score': 0.5598533153533936},\n",
       " {'label': 'LABEL_0', 'score': 0.525170624256134},\n",
       " {'label': 'LABEL_0', 'score': 0.5278325080871582},\n",
       " {'label': 'LABEL_0', 'score': 0.5544049143791199},\n",
       " {'label': 'LABEL_0', 'score': 0.5386032462120056},\n",
       " {'label': 'LABEL_0', 'score': 0.5335230231285095},\n",
       " {'label': 'LABEL_0', 'score': 0.5207797288894653},\n",
       " {'label': 'LABEL_0', 'score': 0.62347811460495},\n",
       " {'label': 'LABEL_0', 'score': 0.5670425295829773},\n",
       " {'label': 'LABEL_0', 'score': 0.569280743598938},\n",
       " {'label': 'LABEL_0', 'score': 0.5732023119926453},\n",
       " {'label': 'LABEL_0', 'score': 0.5244420170783997},\n",
       " {'label': 'LABEL_0', 'score': 0.5609341263771057},\n",
       " {'label': 'LABEL_1', 'score': 0.5042683482170105},\n",
       " {'label': 'LABEL_0', 'score': 0.5081284046173096},\n",
       " {'label': 'LABEL_0', 'score': 0.5296340584754944},\n",
       " {'label': 'LABEL_0', 'score': 0.5298726558685303},\n",
       " {'label': 'LABEL_0', 'score': 0.593966007232666},\n",
       " {'label': 'LABEL_0', 'score': 0.5321632027626038},\n",
       " {'label': 'LABEL_0', 'score': 0.5247842669487},\n",
       " {'label': 'LABEL_0', 'score': 0.5843057036399841},\n",
       " {'label': 'LABEL_0', 'score': 0.5676186680793762},\n",
       " {'label': 'LABEL_0', 'score': 0.5810520052909851},\n",
       " {'label': 'LABEL_0', 'score': 0.5766609311103821},\n",
       " {'label': 'LABEL_0', 'score': 0.5960545539855957},\n",
       " {'label': 'LABEL_0', 'score': 0.5152761936187744},\n",
       " {'label': 'LABEL_0', 'score': 0.5387265682220459},\n",
       " {'label': 'LABEL_0', 'score': 0.5859876871109009},\n",
       " {'label': 'LABEL_0', 'score': 0.5702974200248718},\n",
       " {'label': 'LABEL_0', 'score': 0.6012705564498901},\n",
       " {'label': 'LABEL_0', 'score': 0.5301893353462219},\n",
       " {'label': 'LABEL_0', 'score': 0.5203753709793091},\n",
       " {'label': 'LABEL_0', 'score': 0.5745441913604736},\n",
       " {'label': 'LABEL_0', 'score': 0.5604559779167175},\n",
       " {'label': 'LABEL_0', 'score': 0.5691847205162048},\n",
       " {'label': 'LABEL_0', 'score': 0.5428593754768372},\n",
       " {'label': 'LABEL_0', 'score': 0.5119413733482361},\n",
       " {'label': 'LABEL_0', 'score': 0.5371448397636414},\n",
       " {'label': 'LABEL_0', 'score': 0.5707272291183472},\n",
       " {'label': 'LABEL_1', 'score': 0.5092484951019287},\n",
       " {'label': 'LABEL_1', 'score': 0.502898097038269},\n",
       " {'label': 'LABEL_0', 'score': 0.530239462852478},\n",
       " {'label': 'LABEL_0', 'score': 0.5052375793457031},\n",
       " {'label': 'LABEL_0', 'score': 0.5762383937835693},\n",
       " {'label': 'LABEL_0', 'score': 0.5351410508155823},\n",
       " {'label': 'LABEL_0', 'score': 0.5681270956993103},\n",
       " {'label': 'LABEL_0', 'score': 0.5573020577430725},\n",
       " {'label': 'LABEL_0', 'score': 0.5662603378295898},\n",
       " {'label': 'LABEL_0', 'score': 0.5244925022125244},\n",
       " {'label': 'LABEL_0', 'score': 0.5351755619049072},\n",
       " {'label': 'LABEL_0', 'score': 0.5113164186477661},\n",
       " {'label': 'LABEL_0', 'score': 0.5220946669578552},\n",
       " {'label': 'LABEL_0', 'score': 0.5365455746650696},\n",
       " {'label': 'LABEL_0', 'score': 0.5640785098075867},\n",
       " {'label': 'LABEL_0', 'score': 0.6291010975837708},\n",
       " {'label': 'LABEL_0', 'score': 0.5461995601654053},\n",
       " {'label': 'LABEL_0', 'score': 0.5419285297393799},\n",
       " {'label': 'LABEL_0', 'score': 0.5151240229606628},\n",
       " {'label': 'LABEL_0', 'score': 0.5573011040687561},\n",
       " {'label': 'LABEL_0', 'score': 0.5651534199714661},\n",
       " {'label': 'LABEL_1', 'score': 0.5190511345863342},\n",
       " {'label': 'LABEL_0', 'score': 0.5659325122833252},\n",
       " {'label': 'LABEL_0', 'score': 0.5153926014900208},\n",
       " {'label': 'LABEL_0', 'score': 0.5316638350486755},\n",
       " {'label': 'LABEL_0', 'score': 0.5251254439353943},\n",
       " {'label': 'LABEL_0', 'score': 0.5435237288475037},\n",
       " {'label': 'LABEL_0', 'score': 0.5361733436584473},\n",
       " {'label': 'LABEL_0', 'score': 0.5380255579948425},\n",
       " {'label': 'LABEL_0', 'score': 0.5118739008903503},\n",
       " {'label': 'LABEL_0', 'score': 0.5823397040367126},\n",
       " {'label': 'LABEL_0', 'score': 0.5409643054008484},\n",
       " {'label': 'LABEL_0', 'score': 0.5584761500358582},\n",
       " {'label': 'LABEL_0', 'score': 0.507489800453186},\n",
       " {'label': 'LABEL_0', 'score': 0.5917856693267822},\n",
       " {'label': 'LABEL_0', 'score': 0.5319506525993347},\n",
       " {'label': 'LABEL_0', 'score': 0.540627121925354},\n",
       " {'label': 'LABEL_0', 'score': 0.5632987022399902},\n",
       " {'label': 'LABEL_0', 'score': 0.5439645648002625},\n",
       " {'label': 'LABEL_0', 'score': 0.5356360077857971},\n",
       " {'label': 'LABEL_0', 'score': 0.5248230695724487},\n",
       " {'label': 'LABEL_0', 'score': 0.5922086834907532},\n",
       " {'label': 'LABEL_0', 'score': 0.568018913269043},\n",
       " {'label': 'LABEL_0', 'score': 0.5402547121047974},\n",
       " {'label': 'LABEL_0', 'score': 0.5603681206703186},\n",
       " {'label': 'LABEL_0', 'score': 0.5434002876281738},\n",
       " {'label': 'LABEL_0', 'score': 0.5427938103675842},\n",
       " {'label': 'LABEL_0', 'score': 0.5967658758163452},\n",
       " {'label': 'LABEL_0', 'score': 0.56671142578125},\n",
       " {'label': 'LABEL_0', 'score': 0.58534175157547},\n",
       " {'label': 'LABEL_0', 'score': 0.5604804158210754},\n",
       " {'label': 'LABEL_0', 'score': 0.5688490271568298},\n",
       " {'label': 'LABEL_0', 'score': 0.5374355912208557},\n",
       " {'label': 'LABEL_0', 'score': 0.5648689866065979},\n",
       " {'label': 'LABEL_0', 'score': 0.5537323951721191},\n",
       " {'label': 'LABEL_0', 'score': 0.5400217771530151},\n",
       " {'label': 'LABEL_0', 'score': 0.5309675335884094},\n",
       " {'label': 'LABEL_0', 'score': 0.5074239373207092},\n",
       " {'label': 'LABEL_0', 'score': 0.5306896567344666},\n",
       " {'label': 'LABEL_0', 'score': 0.5374643206596375},\n",
       " {'label': 'LABEL_0', 'score': 0.5449286103248596},\n",
       " {'label': 'LABEL_0', 'score': 0.5539917349815369},\n",
       " {'label': 'LABEL_0', 'score': 0.5270656943321228},\n",
       " {'label': 'LABEL_0', 'score': 0.5340398550033569},\n",
       " {'label': 'LABEL_0', 'score': 0.5618095993995667},\n",
       " {'label': 'LABEL_0', 'score': 0.5188751220703125},\n",
       " {'label': 'LABEL_0', 'score': 0.532218337059021},\n",
       " {'label': 'LABEL_0', 'score': 0.5399602055549622},\n",
       " {'label': 'LABEL_0', 'score': 0.5748081207275391},\n",
       " {'label': 'LABEL_0', 'score': 0.5049858093261719},\n",
       " {'label': 'LABEL_0', 'score': 0.5292422771453857},\n",
       " {'label': 'LABEL_0', 'score': 0.5087645649909973},\n",
       " {'label': 'LABEL_0', 'score': 0.5615097880363464},\n",
       " {'label': 'LABEL_0', 'score': 0.534613311290741}]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f41a6d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
